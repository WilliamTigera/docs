**Supported architecture and versions**

- x86-64
- Linux distribution/kernel:

  - Ubuntu 20.04 and 22.04.
  - Red Hat v8.2 with Linux kernel v4.18.0-193 or above (Red Hat have backported the required features to that build).
  - Another [supported distribution](../getting-started/install-on-clusters/requirements.mdx) with Linux kernel v5.3 or above.

- An underlying network fabric that allows VXLAN traffic between hosts. In eBPF mode, VXLAN is used to forward Kubernetes NodePort traffic.

**Unsupported platforms**

- GKE
- MKE
- TKG
- RKE

:::note

eBPF supports AKS with Calico CNI and {{prodname}} network policy. However, with [AKS with Azure CNI and {{prodname}} network policy](../getting-started/install-on-clusters/aks.mdx#install-with-azure-cni-networking), kube-proxy cannot be disabled so the performance benefits of eBPF are lost. However, there are other reasons to use eBPF other than performance gains, as described in [eBPF use cases](use-cases-ebpf.mdx).

:::

**Unsupported features**

- Clusters with some eBPF nodes and some standard dataplane and/or Windows nodes
- IPv6
- Host endpoint `doNotTrack` policy (other policy types are supported)
- Floating IPs
- SCTP (either for policy or services)
- `Log` action in policy rules
- Tagged VLAN devices
- L7 logs

**Recommendations for performance**

For best pod-to-pod performance, we recommend using an underlying network that doesn't require {{prodname}} to use an overlay. For example:

- A cluster within a single AWS subnet
- A cluster using a compatible cloud provider's CNI (such as the AWS VPC CNI plugin)
- An on-prem cluster with BGP peering configured

If you must use an overlay, we recommend that you use VXLAN, not IPIP. VXLAN has better performance than IPIP in eBPF mode due to various kernel optimizations.
